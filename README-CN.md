# iHuggingfaceHubğŸ˜œ: ä½ è‡ªå·±çš„ç§æœ‰Huggingfaceä»“åº“ï¼
# ç›®æ ‡

ä½¿ç”¨æœ¬åœ°æ–‡ä»¶åˆ›å»ºè‡ªå·±çš„ Huggingface Hub(ç§æœ‰ä»“åº“)æœåŠ¡å™¨ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ Hugging Face Transformers å’Œ Datasets åº“è®¿é—®æœ¬åœ°æ¨¡å‹å’Œæ•°æ®é›†ï¼Œå°±åƒè¿™æ ·ï¼š

```python
import os
from transformers import AutoModelForCausalLM
os.environ['HF_ENDPOINT'] = 'http://server-ip:9999'  
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True).eval()
```

# éƒ¨ç½²

## æœ¬åœ°æ¨¡å¼
```shell
pip3 install -r requirements.txt
python3 app.py
```

## Dockeræ¨¡å¼
```shell
docker compose up -d
```

# ä½¿ç”¨

step1. å°†æ¨¡å‹æ–‡ä»¶æ”¾åˆ°filesç›®å½•

```shell
mkdir -p files/Qwen
cd files/Qwen
git clone https://huggingface.co/Qwen/Qwen-7B-Chat
```

step2. åŠ è½½ç§æœ‰ä»“åº“çš„æ¨¡å‹

```python
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig


def main():
    os.environ['HF_ENDPOINT'] = 'http://127.0.0.1:9999'  # change to app.py host ip
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True).eval()
    generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True,
                                                         resume_download=True)
    model.generation_config = generation_config
    response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
    print(response)


if __name__ == '__main__':
    main()
```

# TODO
* å¢åŠ ç‰ˆæœ¬ï¼ˆrevisionï¼‰æ”¯æŒ
* å¢åŠ æ•°æ®é›†ï¼ˆdatasetï¼‰ æ”¯æŒ